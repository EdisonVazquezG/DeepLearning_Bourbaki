{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "PPO_LLM",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdisonVazquezG/DeepLearning_Bourbaki/blob/main/PPO_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP (Natural Language Processing) with PEFT (Parameter Efficient Fine-Tuning) and LoRA (Low-Rank Adaptation) for Less-Toxic Summarization\n"
      ],
      "metadata": {
        "id": "XU47BN-Wlb7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flujo de trabajo del proyecto:**\n",
        "* **Configuración:** Importar las bibliotecas necesarias y definir los parámetros del proyecto.\n",
        "* **Exploración del conjunto de datos:** Descubrir el conjunto de datos DialogSum.\n",
        "* **Probar la inferencia de cero disparos del modelo:** Inicialmente, probar el modelo FLAN-T5 para la inferencia de cero disparos en tareas de resumen de diálogos para establecer un rendimiento de referencia.\n",
        "* **Preprocesar el diálogo y el resumen del conjunto de datos:** Preprocesar el diálogo y su resumen correspondiente del conjunto de datos para prepararlo para el entrenamiento.\n",
        "* **Realizar un ajuste fino eficiente de parámetros (PEFT):** Implementar el ajuste fino eficiente de parámetros (PEFT), un enfoque de ajuste fino más eficiente que puede reducir significativamente el tiempo de entrenamiento mientras se mantiene el rendimiento.\n",
        "* **Evaluación:**\n",
        "  * Realizar una evaluación humana para medir el resultado del modelo en términos de legibilidad y coherencia. Esto puede implicar que los anotadores clasifiquen los resúmenes generados por calidad.\n",
        "  * Utilizar las métricas ROUGE para evaluar la calidad de los resúmenes generados. ROUGE mide la superposición entre los resúmenes generados y las referencias escritas por humanos."
      ],
      "metadata": {
        "id": "7vavcQdulb7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datos**\n",
        "\n",
        "DialogSum es un conjunto de datos de resumen de diálogos a gran escala, que consta de 13 460 diálogos (más 100 datos de reserva para la generación de temas) con resúmenes y temas correspondientes etiquetados manualmente.\n",
        "\n",
        "[Dialogsum](https://huggingface.co/datasets/knkarthick/dialogsum?row=0)"
      ],
      "metadata": {
        "id": "qYxDSfWTlb7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>1 <span style='color:#78D118'>|</span> Introducción</b>\n",
        "\n",
        "Este proyecto explora las capacidades de los modelos de lenguaje grandes (LLM), haciendo especial hincapié en el uso del ajuste fino con eficiencia de parámetros (PEFT) para crear resúmenes de diálogos con toxicidad reducida. Ajustaremos un modelo FLAN-T5 para generar contenido menos tóxico utilizando el modelo de recompensa por discurso de odio de Meta AI. Este modelo de recompensa es un clasificador binario que predice si un texto determinado es “no odioso” o “odio”. Utilizaremos el Proximal Policy Optimization (PPO) para ajustar el modelo y reducir su toxicidad.\n",
        "\n",
        "Nuestro objetivo principal es mejorar la calidad de los resúmenes de diálogos y, al mismo tiempo, minimizar la toxicidad. Para lograrlo, aplicamos el Proximal Policy Optimization (PPO) para el ajuste fino, con el objetivo de mitigar la salida tóxica del modelo. Además, mostraremos las ventajas del ajuste fino con eficiencia de parámetros (PEFT), demostrando que sus beneficios superan cualquier posible compensación menor en el rendimiento.\n",
        "\n",
        "\n",
        "\n",
        "**NOTA**: Este es un ejemplo y no utilizamos la totalidad de los datos utilizados."
      ],
      "metadata": {
        "id": "uxjRdbgMlb7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:0561c88b-b3ce-41a7-b675-599a809248c8.png)!"
      ],
      "metadata": {
        "id": "xZDjw5wUlb7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:6918531a-d023-4582-9822-d69d250626f0.png)\n"
      ],
      "metadata": {
        "id": "0lyNUR42lb7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:b0c3870e-c7bc-4785-b87c-10dc1c140764.png)"
      ],
      "metadata": {
        "id": "preHX_yDlb7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:61a44672-d5bb-460a-a15a-16236f438ec5.png)"
      ],
      "metadata": {
        "id": "Cug4Qboelb7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install torch\n",
        "%pip install torchdata\n",
        "\n",
        "%pip install transformers\n",
        "%pip install evaluate\n",
        "%pip install rouge_score\n",
        "%pip install peft\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:08:56.701395Z",
          "iopub.execute_input": "2024-11-21T01:08:56.701767Z",
          "iopub.status.idle": "2024-11-21T01:09:43.314927Z",
          "shell.execute_reply.started": "2024-11-21T01:08:56.701706Z",
          "shell.execute_reply": "2024-11-21T01:09:43.31378Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "sQC2m4PIlb7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install the libraries\n",
        "#%pip install --upgrade pip\n",
        "#%pip install --disable-pip-version-check \\\n",
        "    #torch==1.13.1 \\\n",
        "    #torchdata==0.5.1 --quiet\n",
        "\n",
        "#%pip install \\\n",
        "    #transformers==4.27.2 \\\n",
        "    #evaluate==0.4.0 \\\n",
        "    #rouge_score==0.1.2 \\\n",
        "    #peft==0.3.0 --quiet\n",
        "\n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "#%pip install git+https://github.com/lvwerra/trl.git@25fa1bd\n",
        "#!pip install trl==0.4.4 #for PPO\n",
        "\n",
        "#!pip install loralib==0.1.1\n",
        "\n",
        "# Installing the Reinforcement Learning library directly from github.\n",
        "#%pip install git+https://github.com/lvwerra/trl.git@25fa1bd\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "o-q34THGlb7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:09:49.991811Z",
          "iopub.execute_input": "2024-11-21T01:09:49.992707Z",
          "iopub.status.idle": "2024-11-21T01:09:53.905638Z",
          "shell.execute_reply.started": "2024-11-21T01:09:49.99267Z",
          "shell.execute_reply": "2024-11-21T01:09:53.904531Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "NsikQ76ulb7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install trl==0.11.3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:09:56.259482Z",
          "iopub.execute_input": "2024-11-21T01:09:56.259834Z",
          "iopub.status.idle": "2024-11-21T01:10:01.066626Z",
          "shell.execute_reply.started": "2024-11-21T01:09:56.259802Z",
          "shell.execute_reply": "2024-11-21T01:10:01.065649Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "8ip3ycyFlb7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the libs\n",
        "#,GenerationConfig va en transformer\n",
        "\n",
        "from datasets import  load_dataset, Dataset\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, GenerationConfig,Trainer\n",
        "#trl: Transformer Reinforcement Learning Library\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
        "from trl import create_reference_model\n",
        "from trl.core import LengthSampler\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#tqdm library makes the loops show a smart progress meter\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:10:03.143709Z",
          "iopub.execute_input": "2024-11-21T01:10:03.144165Z",
          "iopub.status.idle": "2024-11-21T01:10:20.350623Z",
          "shell.execute_reply.started": "2024-11-21T01:10:03.144122Z",
          "shell.execute_reply": "2024-11-21T01:10:20.349911Z"
        },
        "trusted": true,
        "id": "1soxAz4wlb7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>2 <span style='color:#78D118'>|</span> Descarga de datos</b>"
      ],
      "metadata": {
        "id": "WmrkrN7xlb7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí, utilizaremos el modelo T5 como base entrenada previamente y utilizaremos el tokenizador correspondiente. Puede utilizar un modelo entrenado previamente diferente (y el tokenizador correspondiente) cambiando el nombre del modelo a continuación por un modelo diferente en Hugging Face Hub, o utilizar un modelo personalizado/entrenar un tokenizador desde cero en su propio conjunto de datos. Tenga en cuenta que necesitará muchos más datos y cálculos para entrenar un buen modelo desde cero.\n",
        "\n",
        "T5 is available in multiple sizes, including: T5 Small, T5 Base, T5 Large, T5 3B, T5 11B"
      ],
      "metadata": {
        "id": "SbMFJmrrlb7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset_original = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "# Check the dataset\n",
        "print(dataset_original)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:10:25.462206Z",
          "iopub.execute_input": "2024-11-21T01:10:25.463103Z",
          "iopub.status.idle": "2024-11-21T01:10:28.368254Z",
          "shell.execute_reply.started": "2024-11-21T01:10:25.463066Z",
          "shell.execute_reply": "2024-11-21T01:10:28.367414Z"
        },
        "trusted": true,
        "id": "aLX0iEjmlb7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>3 <span style='color:#78D118'>|</span> Metodos</b>"
      ],
      "metadata": {
        "id": "nMXqA1O8lb7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:10:38.394458Z",
          "iopub.execute_input": "2024-11-21T01:10:38.39494Z",
          "iopub.status.idle": "2024-11-21T01:10:38.401299Z",
          "shell.execute_reply.started": "2024-11-21T01:10:38.394892Z",
          "shell.execute_reply": "2024-11-21T01:10:38.400096Z"
        },
        "trusted": true,
        "id": "8-RAeJN1lb7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4<span style='color:#78D118'>|</span> Tokenizando la información</b>\n"
      ],
      "metadata": {
        "id": "UQlA_AjAlb7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso implica el preprocesamiento del conjunto de datos. Seleccionaremos un subconjunto de los datos, filtraremos los diálogos a una longitud específica para garantizar la legibilidad manteniendo al mismo tiempo un contenido significativo y luego integraremos cada diálogo con una instrucción antes de convertir en tokens las indicaciones. Los identificadores de token resultantes se almacenarán en el campo `input_ids`, mientras que las indicaciones decodificadas se guardarán en el campo `query`.\n",
        "\n",
        "Para agilizar este proceso, es recomendable crear una función llamada `build_dataset`. Esta función se puede definir de la siguiente manera:"
      ],
      "metadata": {
        "id": "YlVKpcnDlb7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(model_name,\n",
        "                  dataset_name,\n",
        "                  input_min_text_length,\n",
        "                  input_max_text_length):\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocess the dataset and split it into train and test parts.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Tokenizer model name.\n",
        "    - dataset_name (str): Name of the dataset to load.\n",
        "    - input_min_text_length (int): Minimum length of the dialogues.\n",
        "    - input_max_text_length (int): Maximum length of the dialogues.\n",
        "\n",
        "    Returns:\n",
        "    - dataset_splits (datasets.dataset_dict.DatasetDict): Preprocessed dataset containing train and test parts.\n",
        "    \"\"\"\n",
        "\n",
        "    # load dataset (only \"train\" part will be enough for this lab).\n",
        "    dataset = load_dataset(dataset_name, split=\"train\")\n",
        "\n",
        "    # Filter the dialogues of length between input_min_text_length and input_max_text_length characters.\n",
        "    dataset = dataset.filter(lambda x: len(x[\"dialogue\"]) > input_min_text_length and len(x[\"dialogue\"]) <= input_max_text_length, batched=False)\n",
        "\n",
        "    # Prepare tokenizer. Setting device_map=\"auto\" allows to switch between GPU and CPU automatically.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name) #, device_map=\"auto\"\n",
        "\n",
        "    def tokenize(sample):\n",
        "\n",
        "        # Wrap each dialogue with the instruction.\n",
        "        prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{sample[\"dialogue\"]}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "        sample[\"input_ids\"] = tokenizer.encode(prompt)\n",
        "\n",
        "        # This must be called \"query\", which is a requirement of our PPO library.\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    # Tokenize each dialogue.\n",
        "    dataset = dataset.map(tokenize, batched=False)\n",
        "    dataset.set_format(type=\"torch\")\n",
        "\n",
        "    # Split the dataset into train and test parts.\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.2, shuffle=False, seed=42)\n",
        "\n",
        "    return dataset_splits\n",
        "\n",
        "dataset = build_dataset(model_name=model_name,\n",
        "                        dataset_name=huggingface_dataset_name,\n",
        "                        input_min_text_length=200,\n",
        "                        input_max_text_length=1000)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:10:40.673866Z",
          "iopub.execute_input": "2024-11-21T01:10:40.674739Z",
          "iopub.status.idle": "2024-11-21T01:11:01.630125Z",
          "shell.execute_reply.started": "2024-11-21T01:10:40.674703Z",
          "shell.execute_reply": "2024-11-21T01:11:01.629246Z"
        },
        "trusted": true,
        "id": "_AilhDYhlb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>5 <span style='color:#78D118'>|</span>  Modelo FLAN-T5 optimizado con instrucciones de resumen</b>\n",
        "\n",
        "## <b>5.1 <span style='color:#78D118'>|</span>  Mejora del modelo FLAN-T5 optimizado con un adaptador de resumen</b>\n",
        "\n",
        "Estamos mejorando el modelo FLAN-T5 original agregando un adaptador de resumen. Este adaptador está diseñado para mejorar el rendimiento del modelo en tareas de resumen.\n",
        "\n",
        "Comenzamos configurando el adaptador utilizando los siguientes parámetros:\n",
        "- `r`: Rank, which is set to 32.\n",
        "- `lora_alpha`: LORA alpha value, set to 32.\n",
        "- `target_modules`: We specify the target modules as [\"q\", \"v\"].\n",
        "- `lora_dropout`: Dropout rate for LORA, set to 0.05.\n",
        "- `bias`: We use \"none\" as the bias configuration.\n",
        "- `task_type`: The task type is set to SEQ_2_SEQ_LM, which is suitable for FLAN-T5.\n",
        "\n",
        "A continuación, cargamos el modelo FLAN-T5 previamente entrenado y creamos una instancia de AutoModelForSeq2SeqLM con el nombre de modelo y el tipo de datos especificados (torch_dtype).\n",
        "\n",
        "También creamos un PeftModel incorporando el modelo cargado previamente.\n",
        "Además, proporcionamos la configuración de LORA, el tipo de datos torch, el mapeo del dispositivo y especificamos que el modelo se puede entrenar."
      ],
      "metadata": {
        "id": "kr8WgBn7lb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,\n",
        "                                              torch_dtype=torch.bfloat16)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(model,\n",
        "                                       'z7ye/peft-dialogue-summary-checkpoint',\n",
        "                                       lora_config=lora_config,\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=True) #device_map=\"auto\",\n",
        "\n",
        "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:11:32.408114Z",
          "iopub.execute_input": "2024-11-21T01:11:32.408494Z",
          "iopub.status.idle": "2024-11-21T01:11:41.696934Z",
          "shell.execute_reply.started": "2024-11-21T01:11:32.408461Z",
          "shell.execute_reply": "2024-11-21T01:11:41.695901Z"
        },
        "trusted": true,
        "id": "ZDOCA1Ovlb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>5.2 <span style='color:#78D118'>|</span> Mejorar el resumen de LLM con aprendizaje de refuerzo con POO</b>\n",
        "\n",
        "Ahora, estamos en el proceso de preparación para el ajuste fino del modelo de lenguaje (LLM) mediante aprendizaje por refuerzo (RL). Aunque se trata de una explicación más detallada del RL, nuestro enfoque actual está en la configuración del modelo de optimización de política proximal (PPO).\n",
        "\n",
        "Este modelo PPO recibirá el modelo PEFT ajustado por instrucción como entrada y se utilizará para optimizar la política de RL de acuerdo con el modelo de recompensa."
      ],
      "metadata": {
        "id": "ecu43wFwlb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,\n",
        "                                                               torch_dtype=torch.bfloat16,\n",
        "                                                               is_trainable=True)\n",
        "\n",
        "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
        "print(ppo_model.v_head)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:11:49.267801Z",
          "iopub.execute_input": "2024-11-21T01:11:49.268616Z",
          "iopub.status.idle": "2024-11-21T01:11:49.318266Z",
          "shell.execute_reply.started": "2024-11-21T01:11:49.268582Z",
          "shell.execute_reply": "2024-11-21T01:11:49.317428Z"
        },
        "trusted": true,
        "id": "HeaC1plrlb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_model = create_reference_model(ppo_model)\n",
        "\n",
        "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:13:18.072049Z",
          "iopub.execute_input": "2024-11-21T01:13:18.072957Z",
          "iopub.status.idle": "2024-11-21T01:13:18.417616Z",
          "shell.execute_reply.started": "2024-11-21T01:13:18.072921Z",
          "shell.execute_reply": "2024-11-21T01:13:18.416678Z"
        },
        "trusted": true,
        "id": "5C_UHXP9lb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>6<span style='color:#78D118'>|</span> Construcción de un modelo de recompensa para el aprendizaje por refuerzo</b>\n",
        "\n",
        "El aprendizaje por refuerzo (RL) es una rama fundamental del aprendizaje automático en la que los agentes toman decisiones dentro de un entorno para maximizar sus recompensas acumuladas. El comportamiento de estos agentes está regido por una política de toma de decisiones y el objetivo fundamental del RL es que el agente adquiera una política óptima o casi óptima que maximice la función de recompensa.\n",
        "\n",
        "Anteriormente, la política original se basaba en el modelo PEFT de instrucciones, esencialmente, el modelo de lenguaje (LLM) antes de someterse a la desintoxicación. Si bien un enfoque implicaba solicitar a los etiquetadores humanos que proporcionaran comentarios sobre la toxicidad de los resultados del modelo, este proceso puede volverse prohibitivamente costoso cuando se aplica durante toda la fase de ajuste fino. Una solución pragmática para evitar este gasto es implementar un modelo de recompensa que aliente al agente a producir resúmenes de diálogo desintoxicados.\n",
        "\n",
        "Un enfoque sensato en este caso es realizar un análisis de sentimientos sobre los resultados del modelo, clasificándolos en dos categorías: \"nothate\" y \"hate\". Se asignan recompensas más altas cuando la probabilidad de clasificar un resultado como \"nothate\" es mayor.\n",
        "\n",
        "En este contexto, emplearemos el [Meta AI's RoBERTa-based hate speech model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) como nuestro modelo de recompensa. Este modelo genera **logits** y, posteriormente, predice probabilidades para dos clases: \"nothate\" y \"hate\". Las recompensas positivas se derivan de los logits asociados con la clase \"nothate\". El modelo se someterá a un ajuste adicional mediante la optimización de políticas proximales (PPO) con estos valores de recompensa.\n",
        "\n",
        "## <b>6.1<span style='color:#78D118'>|</span> Cargue el modelo de discurso de odio basado en RoBERTa de Meta AI</b>"
      ],
      "metadata": {
        "id": "GGxbGqE9lb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
        "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name) # device_map=\"auto\"\n",
        "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name) #, device_map=\"auto\"\n",
        "print(toxicity_model.config.id2label)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:18:21.600343Z",
          "iopub.execute_input": "2024-11-21T01:18:21.601072Z",
          "iopub.status.idle": "2024-11-21T01:18:26.349515Z",
          "shell.execute_reply.started": "2024-11-21T01:18:21.601041Z",
          "shell.execute_reply": "2024-11-21T01:18:26.348709Z"
        },
        "trusted": true,
        "id": "o1H8mEJ0lb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tome un texto no tóxico, conviértalo en un token y páselo al modelo. Imprima los logits de salida, las probabilidades y la recompensa correspondiente que se utilizará para el ajuste fino."
      ],
      "metadata": {
        "id": "Qcj0feYFlb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
        "\n",
        "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
        "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "# Print the probabilities for [not hate, hate]\n",
        "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "# get the logits for \"not hate\" - this is the reward!\n",
        "not_hate_index = 0\n",
        "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "print(f'reward (high): {nothate_reward}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:19:43.781139Z",
          "iopub.execute_input": "2024-11-21T01:19:43.782006Z",
          "iopub.status.idle": "2024-11-21T01:19:43.99509Z",
          "shell.execute_reply.started": "2024-11-21T01:19:43.78197Z",
          "shell.execute_reply": "2024-11-21T01:19:43.994186Z"
        },
        "trusted": true,
        "id": "uKc77M2_lb7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a mostrar un comentario tóxico. Este tendrá una recompensa baja porque es más tóxico."
      ],
      "metadata": {
        "id": "2Bk0Sm5Slb7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_text = \"#Person 1# tells Tommy that the movie was terrible, dumb and stupid.\"\n",
        "\n",
        "toxicity_input_ids = toxicity_tokenizer(toxic_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "logits = toxicity_model(toxicity_input_ids).logits\n",
        "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
        "\n",
        "# Print the probabilities for [not hate, hate]\n",
        "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "print(f'probabilities [not hate, hate]: {probabilities}')\n",
        "\n",
        "# Get the logits for \"not hate\" - this is the reward!\n",
        "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
        "print(f'reward (low): {nothate_reward}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:20:53.003858Z",
          "iopub.execute_input": "2024-11-21T01:20:53.004594Z",
          "iopub.status.idle": "2024-11-21T01:20:53.080751Z",
          "shell.execute_reply.started": "2024-11-21T01:20:53.00456Z",
          "shell.execute_reply": "2024-11-21T01:20:53.079843Z"
        },
        "trusted": true,
        "id": "-puZVfpwlb7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>6.2<span style='color:#78D118'>|</span> Configurar el modelo de recompensa por toxicidad de Pipeline</b>"
      ],
      "metadata": {
        "id": "nj61twSnlb7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurar la canalización de inferencia de Hugging Face para simplificar el código para el modelo de recompensa de toxicidad:"
      ],
      "metadata": {
        "id": "nYXxVMWPlb7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device = 0 if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
        "                          model=toxicity_model_name,\n",
        "                          framework='pt'\n",
        "                          ) #device=device\n",
        "reward_logits_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "reward_probabilities_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "print(\"Reward model output:\")\n",
        "print(\"For non-toxic text\")\n",
        "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n",
        "print(\"For toxic text\")\n",
        "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:26:20.17788Z",
          "iopub.execute_input": "2024-11-21T01:26:20.178964Z",
          "iopub.status.idle": "2024-11-21T01:26:20.762918Z",
          "shell.execute_reply.started": "2024-11-21T01:26:20.178929Z",
          "shell.execute_reply": "2024-11-21T01:26:20.761946Z"
        },
        "trusted": true,
        "id": "FiHTTd1plb7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados son los logits de las clases `nothate` (positiva) y `hate` (negativa). Pero PPO utilizará los logits solo de la clase `nothate` como señal de recompensa positiva utilizada para ayudar a desintoxicar los resultados de LLM."
      ],
      "metadata": {
        "id": "pCMg5oHHlb7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:27:19.840199Z",
          "iopub.execute_input": "2024-11-21T01:27:19.840947Z",
          "iopub.status.idle": "2024-11-21T01:27:19.959203Z",
          "shell.execute_reply.started": "2024-11-21T01:27:19.840914Z",
          "shell.execute_reply": "2024-11-21T01:27:19.95829Z"
        },
        "trusted": true,
        "id": "hQk6cW_zlb7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment_pipe(toxic_text, **reward_logits_kwargs))\n",
        "print(sentiment_pipe(toxic_text, **reward_probabilities_kwargs))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:27:22.099703Z",
          "iopub.execute_input": "2024-11-21T01:27:22.100046Z",
          "iopub.status.idle": "2024-11-21T01:27:22.218325Z",
          "shell.execute_reply.started": "2024-11-21T01:27:22.100016Z",
          "shell.execute_reply": "2024-11-21T01:27:22.217382Z"
        },
        "trusted": true,
        "id": "g8BAp8Tllb7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>6.3<span style='color:#78D118'>|</span> Evaluar la toxicidad</b>\n",
        "\n",
        "Para evaluar el desempeño del modelo tanto antes como después de los procesos de ajuste y desintoxicación, es esencial establecer la métrica de evaluación de toxicidad. La puntuación de toxicidad se representa como un valor decimal que va de 0 a 1, donde 1 significa el grado más alto de toxicidad."
      ],
      "metadata": {
        "id": "kmepxjfQlb7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
        "                                    toxicity_model_name,\n",
        "                                    module_type=\"measurement\",\n",
        "                                    toxic_label=\"hate\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:28:12.09083Z",
          "iopub.execute_input": "2024-11-21T01:28:12.091187Z",
          "iopub.status.idle": "2024-11-21T01:28:12.945243Z",
          "shell.execute_reply.started": "2024-11-21T01:28:12.091155Z",
          "shell.execute_reply": "2024-11-21T01:28:12.944407Z"
        },
        "trusted": true,
        "id": "2XV1srhFlb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intente calcular la toxicidad para las mismas oraciones que en la sección [2.2](#2.2). No sorprende que los puntajes de toxicidad sean las probabilidades de la clase \"odio\" devueltas directamente del modelo de recompensa."
      ],
      "metadata": {
        "id": "8TnvmGAylb71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
        "    non_toxic_text\n",
        "])\n",
        "\n",
        "print(\"Toxicity score for non-toxic text:\")\n",
        "print(toxicity_score[\"toxicity\"])\n",
        "\n",
        "toxicity_score = toxicity_evaluator.compute(predictions=[\n",
        "    toxic_text\n",
        "])\n",
        "\n",
        "print(\"\\nToxicity score for toxic text:\")\n",
        "print(toxicity_score[\"toxicity\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:30:14.783154Z",
          "iopub.execute_input": "2024-11-21T01:30:14.783581Z",
          "iopub.status.idle": "2024-11-21T01:30:14.937689Z",
          "shell.execute_reply.started": "2024-11-21T01:30:14.783549Z",
          "shell.execute_reply": "2024-11-21T01:30:14.936683Z"
        },
        "trusted": true,
        "id": "7Ex6RQ0Ulb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este evaluador se puede utilizar de forma eficaz para calcular los niveles de toxicidad de los diálogos.\n",
        "\n",
        "Para lograrlo, deberá proporcionar varios componentes esenciales, incluido el conjunto de datos de prueba (`dataset[\"test\"]`), el tokenizador utilizado en la sección antes mencionada, el modelo PEFT previamente congelado y el propio evaluador de toxicidad. Para un enfoque simplificado y organizado, se recomienda encapsular estos procedimientos necesarios dentro de una función dedicada denominada `evaluate_toxicity`."
      ],
      "metadata": {
        "id": "tEspydLYlb71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_toxicity(model,\n",
        "                      toxicity_evaluator,\n",
        "                      tokenizer,\n",
        "                      dataset,\n",
        "                      num_samples):\n",
        "\n",
        "    \"\"\"\n",
        "    Preprocess the dataset and split it into train and test parts.\n",
        "\n",
        "    Parameters:\n",
        "    - model (trl model): Model to be evaluated.\n",
        "    - toxicity_evaluator (evaluate_modules toxicity metrics): Toxicity evaluator.\n",
        "    - tokenizer (transformers tokenizer): Tokenizer to be used.\n",
        "    - dataset (dataset): Input dataset for the evaluation.\n",
        "    - num_samples (int): Maximum number of samples for the evaluation.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing two numpy.float64 values:\n",
        "    - mean (numpy.float64): Mean of the samples toxicity.\n",
        "    - std (numpy.float64): Standard deviation of the samples toxicity.\n",
        "    \"\"\"\n",
        "\n",
        "    max_new_tokens=100\n",
        "\n",
        "    toxicities = []\n",
        "    input_texts = []\n",
        "    for i, sample in tqdm(enumerate(dataset)):\n",
        "        input_text = sample[\"query\"]\n",
        "\n",
        "        if i > num_samples:\n",
        "            break\n",
        "\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
        "                                             top_k=0.0,\n",
        "                                             top_p=1.0,\n",
        "                                             do_sample=True)\n",
        "\n",
        "        response_token_ids = model.generate(input_ids=input_ids,\n",
        "                                            generation_config=generation_config)\n",
        "\n",
        "        generated_text = tokenizer.decode(response_token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        toxicity_score = toxicity_evaluator.compute(predictions=[(input_text + \" \" + generated_text)])\n",
        "\n",
        "        toxicities.extend(toxicity_score[\"toxicity\"])\n",
        "\n",
        "    # Compute mean & std using np.\n",
        "    mean = np.mean(toxicities)\n",
        "    std = np.std(toxicities)\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:32:38.170957Z",
          "iopub.execute_input": "2024-11-21T01:32:38.171374Z",
          "iopub.status.idle": "2024-11-21T01:32:38.178425Z",
          "shell.execute_reply.started": "2024-11-21T01:32:38.171319Z",
          "shell.execute_reply": "2024-11-21T01:32:38.177537Z"
        },
        "trusted": true,
        "id": "2bLWFSF6lb71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y ahora realice el cálculo de la toxicidad del modelo antes del ajuste fino/desintoxicación:"
      ],
      "metadata": {
        "id": "CjkRT1l0lb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name) #, device_map=\"auto\"\n",
        "\n",
        "mean_before_detoxification, std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
        "                                                                          toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                          tokenizer=tokenizer,\n",
        "                                                                          dataset=dataset[\"test\"],\n",
        "                                                                          num_samples=10)\n",
        "\n",
        "print(f'toxicity [mean, std] before detox: [{mean_before_detoxification}, {std_before_detoxification}]')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:33:47.235579Z",
          "iopub.execute_input": "2024-11-21T01:33:47.236233Z",
          "iopub.status.idle": "2024-11-21T01:34:23.34564Z",
          "shell.execute_reply.started": "2024-11-21T01:33:47.236199Z",
          "shell.execute_reply": "2024-11-21T01:34:23.344677Z"
        },
        "trusted": true,
        "id": "4naIzfvGlb72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>7 <span style='color:#78D118'>|</span>Realice un ajuste fino para desintoxicar los resúmenes</b>\n",
        "\n",
        "Optimice una política de RL en relación con el modelo de recompensa mediante la optimización de políticas proximales (PPO)."
      ],
      "metadata": {
        "id": "UCZ8aIv1lb72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>7.1 <span style='color:#78D118'>|</span> Inicializar `PPOTrainer`</b>\n",
        "\n",
        "Para la inicialización de `PPOTrainer`, necesitará un collator. En este caso, será una función que transforme los diccionarios de una manera particular. Puede definirlo y probarlo:\n"
      ],
      "metadata": {
        "id": "gZhYuQdolb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
        "print(f'Collator input: {test_data}')\n",
        "print(f'Collator output: {collator(test_data)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:35:40.544469Z",
          "iopub.execute_input": "2024-11-21T01:35:40.54515Z",
          "iopub.status.idle": "2024-11-21T01:35:40.550938Z",
          "shell.execute_reply.started": "2024-11-21T01:35:40.545116Z",
          "shell.execute_reply": "2024-11-21T01:35:40.549866Z"
        },
        "trusted": true,
        "id": "I6lzvBDTlb72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurar los parámetros esenciales. Cargar el `ppo_model` y el tokenizador correspondiente.\n",
        "\n",
        "Además, cargar una versión estática del modelo, denominada `ref_model`.\n",
        "\n",
        "El propósito de tener dos modelos es doble: el primer modelo, `ppo_model`, se somete a optimización, mientras que el segundo modelo, `ref_model`, funciona como un punto de referencia para calcular la divergencia KL a partir del estado inicial.\n",
        "\n",
        "Esto sirve como una señal de recompensa adicional en el proceso de entrenamiento PPO, lo que garantiza que el modelo optimizado no se aleje demasiado del modelo de lenguaje (LLM) original."
      ],
      "metadata": {
        "id": "0E6d7P4olb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1.41e-5\n",
        "max_ppo_epochs=1\n",
        "mini_batch_size=4\n",
        "batch_size=16\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=model_name,\n",
        "    learning_rate=learning_rate,\n",
        "    ppo_epochs=max_ppo_epochs,\n",
        "    mini_batch_size=mini_batch_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "ppo_trainer = PPOTrainer(config=config,\n",
        "                         model=ppo_model,\n",
        "                         ref_model=ref_model,\n",
        "                         tokenizer=tokenizer,\n",
        "                         dataset=dataset[\"train\"],\n",
        "                         data_collator=collator)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:38:45.821186Z",
          "iopub.execute_input": "2024-11-21T01:38:45.82209Z",
          "iopub.status.idle": "2024-11-21T01:38:46.719256Z",
          "shell.execute_reply.started": "2024-11-21T01:38:45.822052Z",
          "shell.execute_reply": "2024-11-21T01:38:46.718561Z"
        },
        "trusted": true,
        "id": "7NYjfp1jlb72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>7.2 <span style='color:#78D118'>|</span> Fine-Tune para el modelo</b>"
      ],
      "metadata": {
        "id": "SDHjacRxlb72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El ciclo de ajuste fino comprende los siguientes pasos clave:\n",
        "\n",
        "1. Recuperar las respuestas de la consulta del modelo de lenguaje de políticas (modelo PEFT).\n",
        "2. Determinar los sentimientos asociados con las consultas y respuestas utilizando el modelo de discurso de odio RoBERTa.\n",
        "3. Optimizar la política utilizando la optimización de políticas proximales (PPO) con el triplete de entradas, que incluye la consulta, la respuesta y la recompensa asociada.\n",
        "\n",
        "Puede confirmar que la operación se está ejecutando correctamente monitoreando las siguientes métricas:\n",
        "\n",
        "- `objective/kl`: Minimización de la divergencia de Kullback-Leibler (KL).\n",
        "- `ppo/returns/mean`: Maximización de los retornos medios.\n",
        "- `ppo/policy/advantages_mean`: Maximización de las ventajas medias.\n",
        "\n",
        "Estas métricas sirven como indicadores del progreso del proceso de capacitación y el logro de objetivos específicos dentro del ciclo de ajuste fino."
      ],
      "metadata": {
        "id": "YIZLnBYElb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length = 100\n",
        "output_max_length = 400\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": 5,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True\n",
        "}\n",
        "\n",
        "reward_kwargs = {\n",
        "    \"top_k\": None, # Return all scores.\n",
        "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
        "    \"batch_size\": 16\n",
        "}\n",
        "\n",
        "max_ppo_steps = 10\n",
        "\n",
        "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
        "    # Break when you reach max_steps.\n",
        "    if step >= max_ppo_steps:\n",
        "        break\n",
        "\n",
        "    prompt_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    # Get response from FLAN-T5/PEFT LLM.\n",
        "    summary_tensors = []\n",
        "\n",
        "    for prompt_tensor in prompt_tensors:\n",
        "        max_new_tokens = output_length_sampler()\n",
        "\n",
        "        generation_kwargs[\"max_new_tokens\"] = max_new_tokens\n",
        "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
        "\n",
        "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
        "\n",
        "    # This needs to be called \"response\".\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
        "\n",
        "    # Compute reward outputs.\n",
        "    query_response_pairs = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    rewards = sentiment_pipe(query_response_pairs, **reward_kwargs)\n",
        "\n",
        "    # You use the `nothate` item because this is the score for the positive `nothate` class.\n",
        "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]\n",
        "\n",
        "    # Run PPO step.\n",
        "    stats = ppo_trainer.step(prompt_tensors, summary_tensors, reward_tensors)\n",
        "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
        "\n",
        "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
        "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
        "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
        "    print('-'.join('' for x in range(100)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:40:58.355028Z",
          "iopub.execute_input": "2024-11-21T01:40:58.355422Z",
          "iopub.status.idle": "2024-11-21T01:46:15.640241Z",
          "shell.execute_reply.started": "2024-11-21T01:40:58.355392Z",
          "shell.execute_reply": "2024-11-21T01:46:15.63942Z"
        },
        "trusted": true,
        "id": "ZwQwnJbRlb72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>7.3 <span style='color:#78D118'>|</span>Evaluar el modelo cuantitativamente</b>\n",
        "\n",
        "Recupere el modelo PPO/PEFT del punto de control del disco guardado y emplee la división del conjunto de datos de prueba para evaluar la puntuación de toxicidad del modelo ajustado por RL."
      ],
      "metadata": {
        "id": "qPVl3yi_lb72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "ppo_model = ppo_model.to(device)\n",
        "ref_model = ref_model.to(device)\n",
        "#toxicity_evaluator = toxicity_evaluator.to(device)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:46:22.76296Z",
          "iopub.execute_input": "2024-11-21T01:46:22.763676Z",
          "iopub.status.idle": "2024-11-21T01:46:23.526095Z",
          "shell.execute_reply.started": "2024-11-21T01:46:22.763641Z",
          "shell.execute_reply": "2024-11-21T01:46:23.525404Z"
        },
        "trusted": true,
        "id": "0zEMPRhulb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model,\n",
        "                                                                        toxicity_evaluator=toxicity_evaluator,\n",
        "                                                                        tokenizer=tokenizer,\n",
        "                                                                        dataset=dataset[\"test\"],\n",
        "                                                                        num_samples=10)\n",
        "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:46:33.226005Z",
          "iopub.execute_input": "2024-11-21T01:46:33.226594Z",
          "iopub.status.idle": "2024-11-21T01:47:06.234407Z",
          "shell.execute_reply.started": "2024-11-21T01:46:33.226557Z",
          "shell.execute_reply": "2024-11-21T01:47:06.233314Z"
        },
        "trusted": true,
        "id": "cx50MsQ_lb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y compare los puntajes de toxicidad del modelo de referencia (antes de la desintoxicación) y el modelo ajustado (después de la desintoxicación)."
      ],
      "metadata": {
        "id": "VA1x4pMmlb73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
        "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
        "\n",
        "print(f'Percentage improvement of toxicity score after detoxification:')\n",
        "print(f'mean: {mean_improvement*100:.2f}%')\n",
        "print(f'std: {std_improvement*100:.2f}%')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:47:16.462617Z",
          "iopub.execute_input": "2024-11-21T01:47:16.462955Z",
          "iopub.status.idle": "2024-11-21T01:47:16.467825Z",
          "shell.execute_reply.started": "2024-11-21T01:47:16.462927Z",
          "shell.execute_reply": "2024-11-21T01:47:16.466971Z"
        },
        "trusted": true,
        "id": "QD_K0IOulb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>7.4 <span style='color:#78D118'>|</span>Evaluar el modelo cualitativamente</b>\n",
        "\n",
        "Explorar ejemplos de muestra del conjunto de datos de prueba, lo que permite una comparación entre el `ref_model` inicial y el `ppo_model` perfeccionado/desintoxicado utilizando el evaluador de toxicidad."
      ],
      "metadata": {
        "id": "QYDd6Gjxlb73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "compare_results = {}\n",
        "\n",
        "df_batch = dataset[\"test\"][0:batch_size]\n",
        "\n",
        "compare_results[\"query\"] = df_batch[\"query\"]\n",
        "prompt_tensors = df_batch[\"input_ids\"]\n",
        "\n",
        "summary_tensors_ref = []\n",
        "summary_tensors = []\n",
        "\n",
        "# Get response from ppo and base model.\n",
        "for i in tqdm(range(batch_size)):\n",
        "    gen_len = output_length_sampler()\n",
        "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "\n",
        "    summary = ref_model.generate(\n",
        "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0),\n",
        "        **generation_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    summary_tensors_ref.append(summary)\n",
        "\n",
        "    summary = ppo_model.generate(\n",
        "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0),\n",
        "        **generation_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    summary_tensors.append(summary)\n",
        "\n",
        "# Decode responses.\n",
        "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
        "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
        "\n",
        "# Sentiment analysis of query/response pairs before/after.\n",
        "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
        "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
        "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
        "\n",
        "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
        "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
        "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:47:39.137289Z",
          "iopub.execute_input": "2024-11-21T01:47:39.137682Z",
          "iopub.status.idle": "2024-11-21T01:49:55.601659Z",
          "shell.execute_reply.started": "2024-11-21T01:47:39.137653Z",
          "shell.execute_reply": "2024-11-21T01:49:55.600663Z"
        },
        "trusted": true,
        "id": "xHNqMADClb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almacenar y revisar los resultados en un DataFrame\n"
      ],
      "metadata": {
        "id": "v1XyPM-Qlb73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', 500)\n",
        "df_compare_results = pd.DataFrame(compare_results)\n",
        "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
        "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
        "df_compare_results_sorted"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-21T01:50:01.036173Z",
          "iopub.execute_input": "2024-11-21T01:50:01.03693Z",
          "iopub.status.idle": "2024-11-21T01:50:01.062958Z",
          "shell.execute_reply.started": "2024-11-21T01:50:01.036894Z",
          "shell.execute_reply": "2024-11-21T01:50:01.062109Z"
        },
        "trusted": true,
        "id": "c0kkr3kNlb73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FuswZEPplb74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}